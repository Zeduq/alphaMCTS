# evaluation/evaluator.py
import numpy as np
import pandas as pd
import random
from typing import Dict, Optional, List

# å¯¼å…¥æ‚¨å¯¼å¸ˆçš„å›æµ‹è„šæœ¬å’Œæˆ‘ä»¬é¡¹ç›®çš„æ¨¡å—
from factor_backtest import FactorBacktest
from utils.data_structures import AlphaFormula, AlphaNode
from agents.critic_agent import CriticAgent
from config import MAX_EVAL_SCORE_PER_DIM, EVAL_TEMP


# ==============================================================================
def get_refinement_dimension(scores: Dict[str, float]) -> str:
    """
    æ ¹æ®åˆ†æ•°é€‰æ‹©ä¸€ä¸ªç»´åº¦è¿›è¡Œä¼˜åŒ–ã€‚
    """
    refinable_dims = {k: v for k, v in scores.items() if k != "Overfitting Risk"}
    if not refinable_dims:
        return random.choice(list(scores.keys()))
    improvement_scores = np.array([MAX_EVAL_SCORE_PER_DIM - v for v in refinable_dims.values()])
    exp_scores = np.exp(improvement_scores / EVAL_TEMP)
    probabilities = exp_scores / (np.sum(exp_scores) + 1e-9)
    return np.random.choice(list(refinable_dims.keys()), p=probabilities)


# ==============================================================================

critic_agent = CriticAgent(prompt_path="prompts/overfitting_assessment.txt")

try:
    tutor_backtester = FactorBacktest()
    print("âœ… å¯¼å¸ˆçš„å›æµ‹å¼•æ“åˆå§‹åŒ–æˆåŠŸã€‚")
except Exception as e:
    print(f"CRITICAL ERROR: åˆå§‹åŒ– FactorBacktest å¤±è´¥ã€‚è¯·ç¡®ä¿ tradelearn åº“é…ç½®æ­£ç¡®ã€‚")
    print(f"Error details: {e}")
    raise


class TutorEvaluator:
    """
    ä¸€ä¸ªå°è£…äº†å¯¼å¸ˆå›æµ‹ä»£ç çš„è¯„ä¼°å™¨ã€‚
    """

    def _get_refinement_history(self, node: AlphaNode) -> str:
        history: List[str] = []
        curr = node
        while curr:
            history.append(f"-> {curr.refinement_summary}")
            curr = curr.parent
        return "\n".join(reversed(history))

    def _translate_metrics_to_scores(self, metrics: pd.Series) -> Dict[str, float]:
        scores = {}
        rank_ic = metrics.get('ic_rank_mean', 0.0)
        turnover = metrics.get('turnover_mean', 1.0)
        sharpe = metrics.get('sharpe', 0.0)
        scores["Effectiveness"] = max(0, min(10, abs(rank_ic) * 200))
        scores["Stability"] = max(0, min(10, (sharpe + 2) * 2.5))
        scores["Turnover"] = max(0, min(10, (1 - min(turnover, 1.0)) * 10))
        scores["Diversity"] = round(np.random.uniform(4.0, 10.0), 2)
        return scores

    def evaluate(self, formula: AlphaFormula, node: AlphaNode) -> Optional[Dict[str, float]]:
        expression = formula.to_expression_string()
        print(f"âš™ï¸  æ­£åœ¨ä½¿ç”¨å¯¼å¸ˆçš„å›æµ‹å¼•æ“è¯„ä¼°Alpha: {expression}")

        try:
            factor_name = formula.name
            # === FIX: Corrected the method name here ===
            eval_metrics = tutor_backtester.run_backtest_by_eval(expression, factor_name)

            if eval_metrics is None or eval_metrics.empty:
                print("âš ï¸ è­¦å‘Š: å›æµ‹å¼•æ“è¿”å›ç©ºç»“æœã€‚")
                return None

            scores = self._translate_metrics_to_scores(eval_metrics)
            history_str = self._get_refinement_history(node)
            critic_output = critic_agent.execute(formula=formula, history=history_str)
            if critic_output and 'score' in critic_output:
                scores["Overfitting Risk"] = float(critic_output.get('score', 5.0))
            else:
                scores["Overfitting Risk"] = 5.0

            print(
                f"ğŸ“Š è¯„ä¼°å®Œæˆ. RankIC: {eval_metrics.get('ic_rank_mean', 'N/A'):.4f}, Sharpe: {eval_metrics.get('sharpe', 'N/A'):.4f}")
            return scores

        except Exception as e:
            print(f"âŒ ä½¿ç”¨å¯¼å¸ˆå¼•æ“è¯„ä¼°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return None